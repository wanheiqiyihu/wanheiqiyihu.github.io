<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wanheiqiyihu.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="py爬虫-b站课程（入门的知识和基础的应用）1.初识requests模块1.1分类通用爬虫：抓取的是一整张页面数据 聚焦爬虫：抓取的是页面中特定的局部内容 增量式爬虫：只会抓取网站中最新更新出来的数据， 1.2http和https1.2.1http协议就是服务器和客户端交互的一种形式 1.2.1.1常用的请求头信息User-Agent:请求载体的身份标识 Connection:请求完毕后，是断开连">
<meta property="og:type" content="article">
<meta property="og:title" content="py爬虫-入门（b站课程）">
<meta property="og:url" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/index.html">
<meta property="og:site_name" content="十三の博客">
<meta property="og:description" content="py爬虫-b站课程（入门的知识和基础的应用）1.初识requests模块1.1分类通用爬虫：抓取的是一整张页面数据 聚焦爬虫：抓取的是页面中特定的局部内容 增量式爬虫：只会抓取网站中最新更新出来的数据， 1.2http和https1.2.1http协议就是服务器和客户端交互的一种形式 1.2.1.1常用的请求头信息User-Agent:请求载体的身份标识 Connection:请求完毕后，是断开连">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122161431973.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122162410799.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122162443268.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122165018639.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122165119672.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122191903352.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122204100206.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122213157511.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126224001577.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126224048649.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126232130118.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126232222195.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126233751717.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126235312766.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126235358881.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126235720349.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127000226816.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127114024171.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127121921265.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127122345118.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127140410577.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127140852085.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127140943953.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127141502235.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127141355498.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127142717471.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127142335945.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127142737442.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127142743267.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127153344429.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127163002697.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127165322367.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127170831634.png">
<meta property="og:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127173730289.png">
<meta property="article:published_time" content="2022-01-27T10:04:33.000Z">
<meta property="article:modified_time" content="2022-01-27T10:09:36.000Z">
<meta property="article:author" content="十三">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122161431973.png">

<link rel="canonical" href="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-cn'
  };
</script>

  <title>py爬虫-入门（b站课程） | 十三の博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">十三の博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-ctf做题笔记">

    <a href="/categories/Experience" rel="section"><i class="download fa-fw"></i>CTF做题笔记</a>

  </li>
        <li class="menu-item menu-item-python">

    <a href="/categories/python" rel="section"><i class="download fa-fw"></i>python</a>

  </li>
        <li class="menu-item menu-item-java">

    <a href="/categories/java" rel="section"><i class="download fa-fw"></i>java</a>

  </li>
        <li class="menu-item menu-item-读书笔记">

    <a href="/categories/readbook" rel="section"><i class="download fa-fw"></i>读书笔记</a>

  </li>
        <li class="menu-item menu-item-渗透测试">

    <a href="/categories/penetration" rel="section"><i class="download fa-fw"></i>渗透测试</a>

  </li>
        <li class="menu-item menu-item-基础">

    <a href="/categories/basic" rel="section"><i class="download fa-fw"></i>基础</a>

  </li>
        <li class="menu-item menu-item-轮子">

    <a href="/categories/lunzi" rel="section"><i class="download fa-fw"></i>轮子</a>

  </li>
        <li class="menu-item menu-item-漏洞库">

    <a href="/categories/fuxian" rel="section"><i class="download fa-fw"></i>漏洞库</a>

  </li>
        <li class="menu-item menu-item-工具">

    <a href="/categories/tools" rel="section"><i class="download fa-fw"></i>工具</a>

  </li>
        <li class="menu-item menu-item-代码审计">

    <a href="/categories/daima" rel="section"><i class="download fa-fw"></i>代码审计</a>

  </li>
        <li class="menu-item menu-item-逆向">

    <a href="/categories/nixiang" rel="section"><i class="download fa-fw"></i>逆向</a>

  </li>
        <li class="menu-item menu-item-应急响应">

    <a href="/categories/yingji" rel="section"><i class="download fa-fw"></i>应急响应</a>

  </li>
        <li class="menu-item menu-item-免杀">

    <a href="/categories/miansha" rel="section"><i class="download fa-fw"></i>免杀</a>

  </li>
        <li class="menu-item menu-item-靶场">

    <a href="/categories/bachang" rel="section"><i class="download fa-fw"></i>靶场</a>

  </li>
        <li class="menu-item menu-item-cve申请">

    <a href="/categories/cve" rel="section"><i class="download fa-fw"></i>cve申请</a>

  </li>
        <li class="menu-item menu-item-c语言">

    <a href="/categories/c" rel="section"><i class="download fa-fw"></i>c语言</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-cn">
    <link itemprop="mainEntityOfPage" href="https://wanheiqiyihu.top/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="十三">
      <meta itemprop="description" content="潜心学习，不骄不躁">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="十三の博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          py爬虫-入门（b站课程）
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-01-27 18:04:33 / 修改时间：18:09:36" itemprop="dateCreated datePublished" datetime="2022-01-27T18:04:33+08:00">2022-01-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">-python</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="py爬虫-b站课程（入门的知识和基础的应用）"><a href="#py爬虫-b站课程（入门的知识和基础的应用）" class="headerlink" title="py爬虫-b站课程（入门的知识和基础的应用）"></a>py爬虫-b站课程（入门的知识和基础的应用）</h1><h1 id="1-初识requests模块"><a href="#1-初识requests模块" class="headerlink" title="1.初识requests模块"></a>1.初识requests模块</h1><h2 id="1-1分类"><a href="#1-1分类" class="headerlink" title="1.1分类"></a>1.1分类</h2><p>通用爬虫：抓取的是一整张页面数据</p>
<p>聚焦爬虫：抓取的是页面中特定的局部内容</p>
<p>增量式爬虫：只会抓取网站中最新更新出来的数据，</p>
<h2 id="1-2http和https"><a href="#1-2http和https" class="headerlink" title="1.2http和https"></a>1.2http和https</h2><h3 id="1-2-1http协议"><a href="#1-2-1http协议" class="headerlink" title="1.2.1http协议"></a>1.2.1http协议</h3><p>就是服务器和客户端交互的一种形式</p>
<h4 id="1-2-1-1常用的请求头信息"><a href="#1-2-1-1常用的请求头信息" class="headerlink" title="1.2.1.1常用的请求头信息"></a>1.2.1.1常用的请求头信息</h4><p>User-Agent:请求载体的身份标识</p>
<p>Connection:请求完毕后，是断开连接还是保持连接。close/keep alive</p>
<h4 id="1-2-1-2常用的响应头信息"><a href="#1-2-1-2常用的响应头信息" class="headerlink" title="1.2.1.2常用的响应头信息"></a>1.2.1.2常用的响应头信息</h4><p>Content-Type:服务器响应回客户端的数据类型</p>
<h3 id="1-2-2https"><a href="#1-2-2https" class="headerlink" title="1.2.2https"></a>1.2.2https</h3><p>安全的超文本传输协议，就是加密之后的http</p>
<p>加密方式：</p>
<p>·对称秘钥加密</p>
<p>·非对称秘钥加密</p>
<p>·证书秘钥加密</p>
<h1 id="2-requests模块"><a href="#2-requests模块" class="headerlink" title="2.requests模块"></a>2.requests模块</h1><h2 id="2-1如何使用"><a href="#2-1如何使用" class="headerlink" title="2.1如何使用"></a>2.1如何使用</h2><p>·指定url</p>
<p>·发起请求</p>
<p>·获取响应数据</p>
<p>·持久化存储</p>
<p>示例1：百度页面的数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&#x27;https://baidu.com/&#x27;</span></span><br><span class="line">res = requests.get(url) <span class="comment">#使用get请求</span></span><br><span class="line"><span class="comment">#返回一个字符串</span></span><br><span class="line"><span class="built_in">print</span>(res.text)</span><br></pre></td></tr></table></figure>

<h2 id="2-2根据案例开始深入学习"><a href="#2-2根据案例开始深入学习" class="headerlink" title="2.2根据案例开始深入学习"></a>2.2根据案例开始深入学习</h2><h3 id="简易的网易采集器（根据关键字在搜索引擎中搜索）"><a href="#简易的网易采集器（根据关键字在搜索引擎中搜索）" class="headerlink" title="简易的网易采集器（根据关键字在搜索引擎中搜索）"></a>简易的网易采集器（根据关键字在搜索引擎中搜索）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">key = <span class="built_in">input</span>()</span><br><span class="line">param = &#123;<span class="string">&#x27;query&#x27;</span>:key&#125;<span class="comment">#可以吧参数封装到字典中</span></span><br><span class="line">url = <span class="string">f&#x27;https://www.sogou.com/web&#x27;</span><span class="comment">#这里的网址是经过简单处理的，观察url就可以发现，关键字在query参数后面。</span></span><br><span class="line"><span class="comment">#ua伪装</span></span><br><span class="line"><span class="comment">#注意这个handers必须是一个字典的形式</span></span><br><span class="line">user_agent = &#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 Edg/97.0.1072.69&#x27;</span>&#125;</span><br><span class="line"><span class="comment">#使用requests的时候用params参数来增加参数</span></span><br><span class="line">res = requests.get(url,params=param,headers=user_agent)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(res.text)</span><br></pre></td></tr></table></figure>

<h3 id="破解百度翻译"><a href="#破解百度翻译" class="headerlink" title="破解百度翻译"></a>破解百度翻译</h3><p>知识：阿贾克斯请求，AJAX 是一种<strong>用于创建快速动态网页的技术</strong>。</p>
<p><strong>通过在后台与服务器进行少量数据交换，AJAX 可以使网页实现异步更新。这意味着</strong>可以在不重新加载整个网页的情况下，对网页的某部分进行更新。</p>
<p>百度翻译每一次翻译都会刷新页面，那我们找到对应的ajax请求就可以了</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122161431973.png" alt="image-20220122161431973"></p>
<p>在分类中的xhr中就是ajax请求的分类。</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122162410799.png" alt="image-20220122162410799"></p>
<p>把他的请求地址作为我们爬虫的请求地址</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122162443268.png" alt="image-20220122162443268"></p>
<p>记住方法是post，我们的请求方法就要变成requests.post,</p>
<p>payload里面只有一个参数kw，我们要在请求的时候加入数据kw<code>data = &#123;&#39;kw&#39;:f&#39;&#123;key&#125;&#39;&#125;</code>记住也是字典模式</p>
<p>响应回来的数据是json数据，我们应该调用json方法返回（返回的是一个字典）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">key = <span class="built_in">input</span>()</span><br><span class="line">url = <span class="string">&#x27;https://fanyi.baidu.com/sug&#x27;</span></span><br><span class="line">handers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 Edg/97.0.1072.69&#x27;</span>&#125;</span><br><span class="line">data = &#123;<span class="string">&#x27;kw&#x27;</span>:<span class="string">f&#x27;<span class="subst">&#123;key&#125;</span>&#x27;</span>&#125;</span><br><span class="line">res = requests.post(url,headers=handers,data=data)</span><br><span class="line"><span class="built_in">print</span>(res.json())</span><br><span class="line"><span class="comment">#进行持久化存储</span></span><br><span class="line">fp = <span class="built_in">open</span>(<span class="string">f&#x27;./<span class="subst">&#123;key&#125;</span>.json&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">json.dump(res.json(),fp = fp,ensure_ascii=<span class="literal">False</span>)<span class="comment">#把数据结构转换成json，因为有中文所以不能用ASCII编码。</span></span><br></pre></td></tr></table></figure>

<h3 id="爬取豆瓣"><a href="#爬取豆瓣" class="headerlink" title="爬取豆瓣"></a>爬取豆瓣</h3><p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122165018639.png" alt="image-20220122165018639"></p>
<p>每一次页面滑到底部的时候，就会出现新的电影，url没有变，那就是阿贾克斯请求，在页面抓包工具中找包</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122165119672.png" alt="image-20220122165119672"></p>
<p>就这一个，是get请求，参数在payload里面去看，记下请求的url</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">num = <span class="built_in">input</span>(<span class="string">&quot;你先要几部恐怖片&quot;</span>)</span><br><span class="line">url = <span class="string">&#x27;https://movie.douban.com/j/chart/top_list?&#x27;</span></span><br><span class="line"><span class="comment">#记住参数要封装在字典中</span></span><br><span class="line">param = &#123;<span class="string">&#x27;type&#x27;</span>:<span class="string">&#x27;20&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;interval_id&#x27;</span>:<span class="string">&#x27;100:90&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;action&#x27;</span>:<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;start&#x27;</span>: <span class="string">&#x27;0&#x27;</span>,<span class="comment">#分析一下我们知道是从第几个电影开始取</span></span><br><span class="line">        <span class="string">&#x27;limit&#x27;</span>:<span class="string">f&#x27;<span class="subst">&#123;num&#125;</span>&#x27;</span>&#125;<span class="comment">#限制取几个电影</span></span><br><span class="line"></span><br><span class="line">handers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 Edg/97.0.1072.69&#x27;</span>&#125;</span><br><span class="line">res = requests.get(url,headers=handers,params=param)</span><br><span class="line"><span class="built_in">print</span>(res.json())</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./恐怖电影.json&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    json.dump(res.json(),fp=fp,ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>然后我又优化了一下，可以选种类，指定电影的数量，输出的时候只输出名字和网址</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[&#x27;纪录片&#x27;, &#x27;音乐&#x27;, 1], [&#x27;剧情&#x27;, &#x27;传记&#x27;, &#x27;历史&#x27;, 2], [&#x27;犯罪&#x27;, &#x27;剧情&#x27;, 3], [&#x27;剧情&#x27;, &#x27;历史&#x27;, &#x27;战争&#x27;, 4], [&#x27;喜剧&#x27;, &#x27;动作&#x27;, &#x27;爱情&#x27;, 5], [&#x27;剧情&#x27;, &#x27;战争&#x27;, &#x27;情色&#x27;, 6], [&#x27;剧情&#x27;, &#x27;歌舞&#x27;, 7]]</span></span><br><span class="line"><span class="string">[[&#x27;剧情&#x27;, &#x27;犯罪&#x27;, &#x27;悬疑&#x27;, 10], [&#x27;犯罪&#x27;, &#x27;剧情&#x27;, 11], [&#x27;剧情&#x27;, &#x27;爱情&#x27;, &#x27;灾难&#x27;, 12], [&#x27;剧情&#x27;, &#x27;爱情&#x27;, &#x27;同性&#x27;, 13], [&#x27;音乐&#x27;, 14], [&#x27;剧情&#x27;, &#x27;科幻&#x27;, &#x27;冒险&#x27;, 15],</span></span><br><span class="line"><span class="string"> [&#x27;剧情&#x27;, &#x27;动画&#x27;, &#x27;奇幻&#x27;, 16], [&#x27;剧情&#x27;, &#x27;科幻&#x27;, &#x27;冒险&#x27;, 17], [&#x27;运动&#x27;, 18], [&#x27;剧情&#x27;, &#x27;犯罪&#x27;, &#x27;惊悚&#x27;, 19]],[[&#x27;悬疑&#x27;, &#x27;惊悚&#x27;, &#x27;恐怖&#x27;, 20]]</span></span><br><span class="line"><span class="string">[[&#x27;剧情&#x27;, &#x27;喜剧&#x27;, &#x27;爱情&#x27;, &#x27;战争&#x27;, 22], [&#x27;科幻&#x27;, &#x27;动画&#x27;, &#x27;短片&#x27;, 23], [&#x27;剧情&#x27;, &#x27;喜剧&#x27;, &#x27;爱情&#x27;, &#x27;战争&#x27;, 24], [&#x27;剧情&#x27;, &#x27;动画&#x27;, &#x27;奇幻&#x27;, 25], [&#x27;剧情&#x27;, &#x27;爱情&#x27;, &#x27;同性&#x27;, 26], [&#x27;冒险&#x27;, &#x27;西部&#x27;, 27],</span></span><br><span class="line"><span class="string"> [&#x27;剧情&#x27;, &#x27;爱情&#x27;, &#x27;家庭&#x27;, 28], [&#x27;爱情&#x27;, &#x27;奇幻&#x27;, &#x27;武侠&#x27;, &#x27;古装&#x27;, 29], [&#x27;剧情&#x27;, &#x27;动画&#x27;, &#x27;奇幻&#x27;, &#x27;古装&#x27;, 30], [&#x27;剧情&#x27;, &#x27;黑色电影&#x27;, 31]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span>)</span><br><span class="line"><span class="built_in">type</span> = <span class="built_in">input</span>(<span class="string">&quot;你要什么类型的电影&quot;</span>)</span><br><span class="line">num = <span class="built_in">input</span>(<span class="string">&quot;你要几部这样的电影，按照评分排序&quot;</span>)</span><br><span class="line">url = <span class="string">&#x27;https://movie.douban.com/j/chart/top_list?&#x27;</span></span><br><span class="line"><span class="comment">#记住参数要封装在字典中</span></span><br><span class="line">param = &#123;<span class="string">&#x27;type&#x27;</span>:<span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">type</span>&#125;</span>&#x27;</span>,<span class="comment">#电影的种类</span></span><br><span class="line">        <span class="string">&#x27;interval_id&#x27;</span>:<span class="string">&#x27;100:90&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;action&#x27;</span>:<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;start&#x27;</span>: <span class="string">&#x27;0&#x27;</span>,<span class="comment">#分析一下我们知道是从第几个电影开始取</span></span><br><span class="line">        <span class="string">&#x27;limit&#x27;</span>:<span class="string">f&#x27;<span class="subst">&#123;num&#125;</span>&#x27;</span>&#125;<span class="comment">#限制取几个电影</span></span><br><span class="line"></span><br><span class="line">handers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 Edg/97.0.1072.69&#x27;</span>&#125;</span><br><span class="line">res = requests.get(url,headers=handers,params=param)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="built_in">int</span>(num)):</span><br><span class="line">    <span class="built_in">print</span>(res.json()[item].get(<span class="string">&#x27;title&#x27;</span>),res.json()[item].get(<span class="string">&#x27;url&#x27;</span>))</span><br><span class="line"><span class="comment">#with open(&#x27;./恐怖电影.json&#x27;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;) as fp:</span></span><br><span class="line"> <span class="comment">#   json.dump(res.json(),fp=fp,ensure_ascii=False)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#[[&#x27;纪录片&#x27;, &#x27;音乐&#x27;, 1], [&#x27;剧情&#x27;, &#x27;传记&#x27;, &#x27;历史&#x27;, 2], [&#x27;犯罪&#x27;, &#x27;剧情&#x27;, 3], [&#x27;剧情&#x27;, &#x27;历史&#x27;, &#x27;战争&#x27;, 4], [&#x27;喜剧&#x27;, &#x27;动作&#x27;, &#x27;爱情&#x27;, 5], [&#x27;剧情&#x27;, &#x27;战争&#x27;, &#x27;情色&#x27;, 6], [&#x27;剧情&#x27;, &#x27;歌舞&#x27;, 7]]</span></span><br><span class="line"><span class="comment">#[[&#x27;剧情&#x27;, &#x27;犯罪&#x27;, &#x27;悬疑&#x27;, 10], [&#x27;犯罪&#x27;, &#x27;剧情&#x27;, 11], [&#x27;剧情&#x27;, &#x27;爱情&#x27;, &#x27;灾难&#x27;, 12], [&#x27;剧情&#x27;, &#x27;爱情&#x27;, &#x27;同性&#x27;, 13], [&#x27;音乐&#x27;, 14], [&#x27;剧情&#x27;, &#x27;科幻&#x27;, &#x27;冒险&#x27;, 15],</span></span><br><span class="line"><span class="comment"># [&#x27;剧情&#x27;, &#x27;动画&#x27;, &#x27;奇幻&#x27;, 16], [&#x27;剧情&#x27;, &#x27;科幻&#x27;, &#x27;冒险&#x27;, 17], [&#x27;运动&#x27;, 18], [&#x27;剧情&#x27;, &#x27;犯罪&#x27;, &#x27;惊悚&#x27;, 19]],[[&#x27;悬疑&#x27;, &#x27;惊悚&#x27;, &#x27;恐怖&#x27;, 20]]</span></span><br><span class="line"><span class="comment">#[[&#x27;剧情&#x27;, &#x27;喜剧&#x27;, &#x27;爱情&#x27;, &#x27;战争&#x27;, 22], [&#x27;科幻&#x27;, &#x27;动画&#x27;, &#x27;短片&#x27;, 23], [&#x27;剧情&#x27;, &#x27;喜剧&#x27;, &#x27;爱情&#x27;, &#x27;战争&#x27;, 24], [&#x27;剧情&#x27;, &#x27;动画&#x27;, &#x27;奇幻&#x27;, 25], [&#x27;剧情&#x27;, &#x27;爱情&#x27;, &#x27;同性&#x27;, 26], [&#x27;冒险&#x27;, &#x27;西部&#x27;, 27],6.</span></span><br><span class="line"><span class="comment"># [&#x27;剧情&#x27;, &#x27;爱情&#x27;, &#x27;家庭&#x27;, 28], [&#x27;爱情&#x27;, &#x27;奇幻&#x27;, &#x27;武侠&#x27;, &#x27;古装&#x27;, 29], [&#x27;剧情&#x27;, &#x27;动画&#x27;, &#x27;奇幻&#x27;, &#x27;古装&#x27;, 30], [&#x27;剧情&#x27;, &#x27;黑色电影&#x27;, 31]]</span></span><br></pre></td></tr></table></figure>

<h3 id="综合：国家药品监督管理总局中基于中华人民共各国化妆品生产许可相关数据（动态加载）"><a href="#综合：国家药品监督管理总局中基于中华人民共各国化妆品生产许可相关数据（动态加载）" class="headerlink" title="综合：国家药品监督管理总局中基于中华人民共各国化妆品生产许可相关数据（动态加载）"></a>综合：国家药品监督管理总局中基于中华人民共各国化妆品生产许可相关数据（动态加载）</h3><p>首先我们发现这下面的数据都是阿贾克斯数据包动态加载出来的</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122191903352.png" alt="image-20220122191903352"></p>
<p>然后每一个公司的详细信息也是动态加载出来的</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122204100206.png" alt="image-20220122204100206"></p>
<p>那就可以开始写代码了，主要思路就是拼接url上的id</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">page = <span class="built_in">input</span>(<span class="string">&quot;请输入你要爬取第几页的信息:  &quot;</span>)</span><br><span class="line">handers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 Edg/97.0.1072.69&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsList&#x27;</span></span><br><span class="line">param = &#123;</span><br><span class="line"><span class="string">&#x27;method&#x27;</span>: <span class="string">&#x27;getXkzsList&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;on&#x27;</span>: <span class="string">&#x27;true&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;page&#x27;</span>: <span class="string">f&#x27;<span class="subst">&#123;page&#125;</span>&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;pageSize&#x27;</span>: <span class="string">&#x27;15&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;productName&#x27;</span>:<span class="string">&#x27;&#x27;</span> ,</span><br><span class="line"><span class="string">&#x27;conditionType&#x27;</span>: <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;applyname&#x27;</span>:<span class="string">&#x27;&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;applysn&#x27;</span>: <span class="string">&#x27;&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#首先获取公司和他们的id</span></span><br><span class="line">res = requests.post(url,headers=handers)</span><br><span class="line">name_id = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(res.json().get(<span class="string">&#x27;list&#x27;</span>))):</span><br><span class="line">    <span class="built_in">id</span> = res.json().get(<span class="string">&#x27;list&#x27;</span>)[i].get(<span class="string">&#x27;ID&#x27;</span>)</span><br><span class="line">    name = res.json().get(<span class="string">&#x27;list&#x27;</span>)[i].get(<span class="string">&#x27;EPS_NAME&#x27;</span>)</span><br><span class="line">    name_id[name] = <span class="built_in">id</span></span><br><span class="line"><span class="keyword">for</span> index,item <span class="keyword">in</span> <span class="built_in">enumerate</span>(name_id.keys()):</span><br><span class="line">    <span class="built_in">print</span>(index,item)</span><br><span class="line"></span><br><span class="line"><span class="comment">#问要看哪个公司的信息</span></span><br><span class="line">company = <span class="built_in">input</span>(<span class="string">&quot;你想看哪个公司的信息，请输入那个公司的名字，懒得输请自己复制，序号功能太难弄了： &quot;</span>)</span><br><span class="line">param2 = &#123;</span><br><span class="line"><span class="string">&#x27;method&#x27;</span>: <span class="string">&#x27;getXkzsById&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;id&#x27;</span>: name_id.get(<span class="string">f&#x27;<span class="subst">&#123;company&#125;</span>&#x27;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">res2 = requests.post(<span class="string">&#x27;http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do&#x27;</span>,headers=handers,params=param2)</span><br><span class="line"><span class="built_in">print</span>(res2.json())</span><br></pre></td></tr></table></figure>

<h1 id="2-数据解析"><a href="#2-数据解析" class="headerlink" title="2.数据解析"></a>2.数据解析</h1><p>聚焦爬虫：爬取页面中指定的页面内容</p>
<p>数据解析分类：</p>
<p>·正则</p>
<p>·bs4</p>
<p>·xpath（重点）</p>
<p>数据解析原理概述：</p>
<p>​    解析的局部的文本内容都会在标签之间或者标签对应的属性中进行存储</p>
<p>​    进行指定标签的定位</p>
<p>​    标签或者标签对应属性中存储的数据值进行提取，这个过程就叫解析。</p>
<h2 id="2-1使用正则来进行数据解析"><a href="#2-1使用正则来进行数据解析" class="headerlink" title="2.1使用正则来进行数据解析"></a>2.1使用正则来进行数据解析</h2><p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220122213157511.png" alt="image-20220122213157511"></p>
<h2 id="2-2bs4解析（python独有的方式）"><a href="#2-2bs4解析（python独有的方式）" class="headerlink" title="2.2bs4解析（python独有的方式）"></a>2.2bs4解析（python独有的方式）</h2><p>bs4数据解析的原理：</p>
<p>1.实例化一个BeautifuSoup的对象，并且将页面源码数据加载到该对象中。</p>
<p>2.通过调用BeautifuSoup对象中相关的属性或者方法进行标签定位和数据提取。</p>
<p><code>pip install bs4</code></p>
<h3 id="2-2-1示例：实例化BeautifulSoup"><a href="#2-2-1示例：实例化BeautifulSoup" class="headerlink" title="2.2.1示例：实例化BeautifulSoup"></a>2.2.1示例：实例化BeautifulSoup</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">fp = <span class="built_in">open</span>(<span class="string">&#x27;./test.thml&#x27;</span>,<span class="string">&#x27;r&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">soup = BeautifulSoup(fp,<span class="string">&#x27;lxml&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>示例二：将互联网上获取的页面源码加载到该对象中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">page_text = page.taxt</span><br><span class="line">soup = BeautifulSoup(page_text,<span class="string">&#x27;lxml&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-2-2相关方法和属性"><a href="#2-2-2相关方法和属性" class="headerlink" title="2.2.2相关方法和属性"></a>2.2.2相关方法和属性</h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/Crush999/p/11921495.html">beautifulSoup常用方法 - Crush999 - 博客园 (cnblogs.com)</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">soup.a<span class="comment">#获取其中的a标签，只获取第一个 soup.tagName,返回第一次出现的那个标签</span></span><br><span class="line">soup.find(<span class="string">&#x27;a&#x27;</span>):<span class="comment">#返回a标签的值</span></span><br><span class="line">soup.find(<span class="string">&#x27;div&#x27;</span>,class_=<span class="string">&#x27;abc&#x27;</span>)<span class="comment">#返回属性是abc的biv标签</span></span><br><span class="line">soup.find_all(<span class="string">&#x27;div&#x27;</span>)<span class="comment">#返回所有的标签是div的标签，就是返回符合标准的所有标签。（列表）</span></span><br><span class="line"></span><br><span class="line">soup.select(<span class="string">&#x27;.asd&#x27;</span>)<span class="comment">#这里面的.是类选择器，这个select函数里面装选择器，返回复合条件的标签（列表）</span></span><br><span class="line">soup.select(<span class="string">&#x27;.asd &gt; url &gt; a&#x27;</span>)[<span class="number">2</span>]<span class="comment">#定位asd类下的url标签中的a标签，返回的是列表，想要哪个标签就用列表的索引。&gt;表示的是一个层级</span></span><br><span class="line">soup.select(<span class="string">&#x27;.asd &gt; url a&#x27;</span>)<span class="comment">#空格表示的是多个层级</span></span><br><span class="line"></span><br><span class="line">soup.a.text/string/get_text()<span class="comment">#后面三个都是获取a标签中的的文本内容的意思</span></span><br><span class="line">text/get_text()<span class="comment">#可以获取某一个标签中所有的文本内容</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="2-3xpath解析"><a href="#2-3xpath解析" class="headerlink" title="2.3xpath解析"></a>2.3xpath解析</h2><p>是最常用最便携的高效解析方式</p>
<p>解析原理：</p>
<p>实例化 etree对象，且需要将被解析的页面源码加载到该对象中。</p>
<p>调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获</p>
<p><code>pip install lxml</code></p>
<p>实例化对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">form lxml <span class="keyword">import</span> etree</span><br><span class="line">tree = etree.parse(filePath)</span><br><span class="line"><span class="keyword">or</span></span><br><span class="line">tree = etree.HTML(<span class="string">&#x27;page_text&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-3-1常用xpath表达式"><a href="#2-3-1常用xpath表达式" class="headerlink" title="2.3.1常用xpath表达式"></a>2.3.1常用xpath表达式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tree.xpath(<span class="string">&#x27;/html/head/title&#x27;</span>)<span class="comment">#在html标签中找head标签中找title标签。返回一个列表，element类型的对象。如果是两个//那就是多个层级</span></span><br><span class="line">上面的表达式就是也可以表示成这样</span><br><span class="line"><span class="comment">#tree.xpath(&#x27;/html//title&#x27;)</span></span><br><span class="line"><span class="comment">#tree.xpath(&#x27;//title&#x27;)这个表示源码中所有的title标签，从任意位置开始定位。</span></span><br><span class="line"></span><br><span class="line">r = tree.xpath(<span class="string">&#x27;//title[@class = &quot;song&quot;]&#x27;</span>)<span class="comment">#实现属性定位</span></span><br><span class="line">r = tree.xpath(<span class="string">&#x27;//title[@class = &quot;song&quot;]/p[3]&#x27;</span>)<span class="comment">#实现索引定位，他这个索引是从一开始的，ε=(´ο｀*)))唉，能不能统一一下啊。注意那个p是标签名字</span></span><br><span class="line"></span><br><span class="line">tree.xpath(<span class="string">&#x27;/html//title/text()&#x27;</span>)<span class="comment">#就可以取到文本内容，返回的是一个列表</span></span><br><span class="line">tree.xpath(<span class="string">&#x27;/html//title//text()&#x27;</span>)<span class="comment">#就可以取到里面所有的文本内容，返回的是一个列表</span></span><br><span class="line">tree.xpath(<span class="string">&#x27;/html//title/@属性名&#x27;</span>)<span class="comment">#取属性</span></span><br><span class="line"></span><br><span class="line">注意在表达式中用|管道符表示或者</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup<span class="comment">#导入模块</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">url=<span class="string">&quot;https://www.unjs.com/lunwen/f/20191111001204_2225087.html&quot;</span></span><br><span class="line">headers= &#123;</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.67 Safari/537.36 Edg/87.0.664.47&quot;</span></span><br><span class="line">&#125;<span class="comment">#模拟的服务器头</span></span><br><span class="line"></span><br><span class="line">html = requests.get(url,headers=headers)</span><br><span class="line">page.encoding = <span class="string">&#x27;utf-8&#x27;</span><span class="comment">#防止乱码</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zhuochuyu7096/article/details/80085321">(36条消息) scrapy爬取小说时换行问题_Keyu-CSDN博客_python爬小说换行</a></p>
<p>验证码绕过的时候就用云打码平台</p>
<p>实例，爬取图片，还有个爬小说的我就不放出来了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;米娜这个东西要开全局代理才能看呢！&quot;</span>)</span><br><span class="line">handers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 Edg/97.0.1072.69&#x27;</span>&#125;</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    page = <span class="built_in">input</span>(<span class="string">&quot;大家好越江又来送好东西了，这次是图片哦，每一页有三十四个系列的图片，最多八十页哦！请输入您要的页数： &quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">int</span>(page) &gt; <span class="number">80</span> <span class="keyword">or</span> <span class="built_in">int</span>(page) &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;不要就不要，乱输什么数字啊！&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;好东西2&quot;</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;正在爬取网址&#x27;</span>)</span><br><span class="line">    os.makedirs(<span class="string">&#x27;好东西2&#x27;</span>)</span><br><span class="line">    pic_dict = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> pagex <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">int</span>(page) + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            url = <span class="string">f&#x27;http://www.52rosi.com/?action=ajax_post&amp;pag=<span class="subst">&#123;pagex&#125;</span>&#x27;</span></span><br><span class="line">            res = requests.get(url,headers=handers)</span><br><span class="line">            res.encoding=<span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">            page_text = res.text</span><br><span class="line">            tree = etree.HTML(page_text)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;pagex / <span class="built_in">int</span>(page) * <span class="number">100</span>&#125;</span>%...正在爬取网址&#x27;</span>)</span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">33</span>):</span><br><span class="line">                pic_nam = tree.xpath(<span class="string">f&#x27;//div[@class=&quot;post-home&quot;][<span class="subst">&#123;index&#125;</span>]//span/a/@title&#x27;</span>)</span><br><span class="line">                pic_name = pic_nam[<span class="number">0</span>].replace(<span class="string">&quot;上的评论&quot;</span>,<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                pic_url = tree.xpath(<span class="string">f&#x27;//div[@class=&quot;post-home&quot;][<span class="subst">&#123;index&#125;</span>]//span/a/@href&#x27;</span>)</span><br><span class="line">                pic_dict[pic_name] = pic_url[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;第<span class="subst">&#123;page&#125;</span>页出错了&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(res.status_code)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./好东西2/各个网址.txt&#x27;</span>,mode=<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(<span class="built_in">str</span>(pic_dict))</span><br><span class="line"></span><br><span class="line">    tip = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> item_name,item_url <span class="keyword">in</span> pic_dict.items():</span><br><span class="line">        tip += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;tip / <span class="built_in">len</span>(pic_dict.keys()) * <span class="number">100</span>&#125;</span>%正在帮你下载图片&quot;</span>)</span><br><span class="line">        os.makedirs(<span class="string">f&#x27;./好东西2/<span class="subst">&#123;item_name&#125;</span>&#x27;</span>)</span><br><span class="line">        res = requests.get(item_url,headers=handers)</span><br><span class="line">        tree = etree.HTML(res.text)</span><br><span class="line">        url_list = tree.xpath(<span class="string">&#x27;//div[@id=&quot;wrap&quot;]//dt[@class=&quot;gallery-icon&quot;]/a/@href&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> index,urlx <span class="keyword">in</span> <span class="built_in">enumerate</span>(url_list):</span><br><span class="line"></span><br><span class="line">            res = requests.get(urlx, headers=handers)</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&quot;./好东西2/<span class="subst">&#123;item_name.strip()&#125;</span>/<span class="subst">&#123;index&#125;</span>.jpg&quot;</span>,mode=<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(res.content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;把当前目录下的“好东西”文件夹删了，我才好给你资源啊！&quot;</span>)</span><br></pre></td></tr></table></figure>



<h1 id="3-高性能异步爬虫"><a href="#3-高性能异步爬虫" class="headerlink" title="3.高性能异步爬虫"></a>3.高性能异步爬虫</h1><h2 id="3-1异步爬虫的方式"><a href="#3-1异步爬虫的方式" class="headerlink" title="3.1异步爬虫的方式"></a>3.1异步爬虫的方式</h2><p>·多线程，多进程（不建议）</p>
<p>·线程池，进程池（建议）</p>
<p>下面线程池的使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># 导入线程池所对应的类</span></span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="comment">#模拟下载</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page</span>(<span class="params"><span class="built_in">str</span></span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;正在下载&quot;</span>, <span class="built_in">str</span>)</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;下载成功&quot;</span>, <span class="built_in">str</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    listx = [<span class="string">&#x27;aa&#x27;</span>,<span class="string">&#x27;ss&#x27;</span>,<span class="string">&#x27;vv&#x27;</span>,<span class="string">&#x27;bb&#x27;</span>]</span><br><span class="line">    <span class="comment">#实例化一个线程池对象</span></span><br><span class="line">    pool = Pool(<span class="number">4</span>)<span class="comment">#创建了四个线程</span></span><br><span class="line">    <span class="comment">#将列表中每一个元素传递个get</span></span><br><span class="line">    pool.<span class="built_in">map</span>(get_page,listx)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://www.freesion.com/article/740970388/">pytorch使用出现”RuntimeError: An attempt has been made to start a new process before the…” 解决方法 - 灰信网（软件开发博客聚合） (freesion.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/liuzhzhao/p/12114453.html">Python中用requests处理cookies的3种方法 - 王昭君的昭 - 博客园 (cnblogs.com)</a></p>
<h2 id="3-2单线程加异步协程"><a href="#3-2单线程加异步协程" class="headerlink" title="3.2单线程加异步协程"></a>3.2单线程加异步协程</h2><h3 id="3-2-1基本概念"><a href="#3-2-1基本概念" class="headerlink" title="3.2.1基本概念"></a>3.2.1基本概念</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41599977/article/details/93656042">(36条消息) python多任务—协程（一）_夜风晚凉的博客-CSDN博客_python 协程</a></p>
<p>event_loop:事件循环</p>
<p>coroutine:协程对象</p>
<p>task:任务</p>
<p>future:代表将来执行或还没有执行的任务，实际上和没有本质区别</p>
<p>asunc:定义一个协程</p>
<p>await:用来挂起阻塞方法的执行。</p>
<h3 id="3-2-2相关操作"><a href="#3-2-2相关操作" class="headerlink" title="3.2.2相关操作"></a>3.2.2相关操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">request</span>(<span class="params">url</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;正在请求的url是<span class="subst">&#123;url&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;请求成功&#x27;</span>)</span><br><span class="line"><span class="comment">#async修饰的函数，调用之后返回的一个协程对象</span></span><br><span class="line"></span><br><span class="line">c = request(<span class="string">&#x27;www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="comment">#创建一个事件循环对象</span></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line"><span class="comment">#将协程对象注册到loop中，然后启动loop</span></span><br><span class="line">loop.run_until_complete(c)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">正在请求的url是www.baidu.com</span></span><br><span class="line"><span class="string">请求成功</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">request</span>(<span class="params">url</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;正在请求的url是<span class="subst">&#123;url&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;请求成功&#x27;</span>)</span><br><span class="line"><span class="comment">#async修饰的函数，调用之后返回的一个协程对象</span></span><br><span class="line"><span class="comment">#task的使用</span></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line"><span class="comment">#基于loop创建了一个task任务对象</span></span><br><span class="line">task = loop.create_task(c)</span><br><span class="line"><span class="built_in">print</span>(task)<span class="comment">#还没有被执行</span></span><br><span class="line"></span><br><span class="line">loop.run_until_complete(task)</span><br><span class="line"><span class="built_in">print</span>(task)<span class="comment">#被执行了</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#future的使用</span></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">task = asyncio.ensure_future(c)</span><br><span class="line">loop.run_until_complete(task)</span><br><span class="line"><span class="built_in">print</span>(task)</span><br></pre></td></tr></table></figure>

<p>绑定回调</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">request</span>(<span class="params">url</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;正在请求的url是<span class="subst">&#123;url&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;请求成功&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> url</span><br><span class="line"><span class="comment">#async修饰的函数，调用之后返回的一个协程对象</span></span><br><span class="line">c = request(<span class="string">&#x27;www.baidu.com&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">callback_func</span>(<span class="params">task</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(task.result())<span class="comment">#result函数返回执行函数的返回值</span></span><br><span class="line"><span class="comment">#绑定回调</span></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">task = asyncio.ensure_future(c)</span><br><span class="line"><span class="comment">#将回调函数绑定到任务对象中</span></span><br><span class="line">task.add_done_callback(callback_func)</span><br><span class="line">loop.run_until_complete(task)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">正在请求的url是www.baidu.com</span></span><br><span class="line"><span class="string">请求成功</span></span><br><span class="line"><span class="string">www.baidu.com</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多任务异步协程</span></span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment">#调用这个async修饰的函数就会返回一个协程对象</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">request</span>(<span class="params">url</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;正在下载&#x27;</span>,url)</span><br><span class="line">    <span class="comment">#在异步协程中如果出现了同步模块相关的代码，那么就无法实现异步</span></span><br><span class="line">    <span class="comment">#time.sleep(2)</span></span><br><span class="line">    <span class="comment">#当在asyncio中遇到阻塞操作必须进行手动挂起</span></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;下载完毕&#x27;</span>,url)</span><br><span class="line">start = time.time()</span><br><span class="line">urls = [</span><br><span class="line">    <span class="string">&#x27;www.baidu.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;www.sogou.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;www.goubanjia.com&#x27;</span></span><br><span class="line">]</span><br><span class="line"><span class="comment">#任务列表，存放多个任务对象</span></span><br><span class="line">stasks = []</span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">    c = request(url)</span><br><span class="line">    <span class="comment">#将协程对象封装到任务对象当中</span></span><br><span class="line">    task = asyncio.ensure_future(c)</span><br><span class="line">    stasks.append(task)</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(asyncio.wait(stasks))<span class="comment">#必须要把这个列表封装到wait方法中，固定写法</span></span><br><span class="line"><span class="built_in">print</span>(time.time()-start)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="3-2-3aiohttp模块"><a href="#3-2-3aiohttp模块" class="headerlink" title="3.2.3aiohttp模块"></a>3.2.3aiohttp模块</h3><p>注意request模块是同步的，在异步协程中就不能使用request模块了，我们要使用基于异步的网络请求模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="comment">#使用该模块中的ClientSession对象</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">    <span class="comment">#get()/post()</span></span><br><span class="line">    <span class="comment">#headers,params/data,proxy = &#x27;http://ip:port&#x27;</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span>  <span class="keyword">await</span> session.get(url) <span class="keyword">as</span> response:<span class="comment">#阻塞操作必须使用await关键字手动挂起</span></span><br><span class="line">        page_text = <span class="keyword">await</span> response.text()<span class="comment">#text方法可以返回字符串形式的响应数据,这里获取响应数据操作之前也必须使用await进行手动挂起</span></span><br><span class="line">        <span class="comment">#read()方法返回的是二进制形式的响应数据</span></span><br><span class="line">        <span class="comment">#json()方法返回的就是json对象</span></span><br><span class="line">        <span class="built_in">print</span>(page_text)</span><br></pre></td></tr></table></figure>

<h1 id="4-selenium模块"><a href="#4-selenium模块" class="headerlink" title="4.selenium模块"></a>4.selenium模块</h1><h2 id="4-1主要作用"><a href="#4-1主要作用" class="headerlink" title="4.1主要作用"></a>4.1主要作用</h2><p>selenium模块可以很便捷的获取网站中动态加载的数据。</p>
<p>selenium也可以十分便捷的实现模拟登陆</p>
<p><code>什么是selenium？</code></p>
<p>基于浏览器自动化的一个模块。</p>
<h2 id="4-2selenium初试"><a href="#4-2selenium初试" class="headerlink" title="4.2selenium初试"></a>4.2selenium初试</h2><p>使用这个模块，还需要一个浏览器的驱动程序</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/61312218">Selenium3 + Python3：安装selenium浏览器驱动 - 知乎 (zhihu.com)</a></p>
<h2 id="4-3常用方法"><a href="#4-3常用方法" class="headerlink" title="4.3常用方法"></a>4.3常用方法</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/linhaifeng/articles/7783599.html">selenium模块 - linhaifeng - 博客园 (cnblogs.com)</a></p>
<p>我不怎么用就偷下懒啦(<em>^▽^</em>)</p>
<h1 id="5-scrapy框架"><a href="#5-scrapy框架" class="headerlink" title="5.scrapy框架"></a>5.scrapy框架</h1><p><code>什么是框架？</code></p>
<p>就是一个集成了很多功能并且具有很强通用性的一个项目模板。</p>
<p><code>如何学习框架？</code></p>
<p>专门学习框架封装的各种功能的详细用法。</p>
<h2 id="5-1环境安装"><a href="#5-1环境安装" class="headerlink" title="5.1环境安装"></a>5.1环境安装</h2><p>windows：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pip install wheel</span><br><span class="line">下载twisted,下载地址为http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</span><br><span class="line">安装twisted,pip install Twisted-17.1.0-cp36-cp36m-win_amd64.whl</span><br><span class="line">pip install pywin32</span><br><span class="line">pip install scrapy</span><br><span class="line">测试：在终端输入scrapy指令，没有报错即表示安装成功！</span><br></pre></td></tr></table></figure>

<h2 id="5-2基本使用"><a href="#5-2基本使用" class="headerlink" title="5.2基本使用"></a>5.2基本使用</h2><p>创建一个工程:用terminal</p>
<p>scrapy startproject xxx</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126224001577.png" alt="image-20220126224001577"></p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126224048649.png" alt="image-20220126224048649"></p>
<p>下面的scrapy.cfg文件就是他的配置文件，里面的spider文件夹我们一般称为爬虫文件夹。</p>
<p>在spider字母里中创建一个爬虫文件</p>
<p>·scrapy genspider spiderName <a target="_blank" rel="noopener" href="http://www.xxx.com/">www.xxx.com</a></p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126232130118.png" alt="image-20220126232130118"></p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126232222195.png" alt="image-20220126232222195"></p>
<p>分析一下创建的爬虫文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpidernameSpider</span>(<span class="params">scrapy.Spider</span>):</span><span class="comment">#他的父类是spider</span></span><br><span class="line">    <span class="comment">#爬虫文件的名称：就是爬虫源文件的一个唯一标识，不能重复</span></span><br><span class="line">    name = <span class="string">&#x27;spiderName&#x27;</span></span><br><span class="line">    <span class="comment">#允许的域名：用来限定start urls列表中那些url可以进行请求发送，但是我们通常情况下我们不会使用这个限制。</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;www.baidu.com&#x27;</span>]</span><br><span class="line">    <span class="comment">#起始的url列表：该列表中存放的url会被scrapy自动进行请求的发送</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.baidu.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#用作于数据解析，response参数表示的就是请求成功后对应的响应对象，parse调用一次只会接受一个调用response</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(response)<span class="comment">#这个是后面我自己加的，实验用的</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>执行工程</p>
<p>scrapy crawl spiderName</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126233751717.png" alt="image-20220126233751717"></p>
<p>执行上述爬虫文件之后会出现一堆日志，但是没有输出response。</p>
<p>重要输出</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126235312766.png" alt="image-20220126235312766"></p>
<p>也就是我们尊从了robots协议的，也就是其他的东西我们就爬不到了，先给他改成false。</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126235358881.png" alt="image-20220126235358881"></p>
<p>但是那么多日志信息打扰了我们的查询，我们用–nolog参数可以消除参数</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220126235720349.png" alt="image-20220126235720349"></p>
<p>但是如果一味的使用nolog的话，我们反而有些时候看不到报错的信息，那该怎么办呢</p>
<p>在setting中加一行 LOG_LEVEL = ‘ERROR’</p>
<p>让我们输出指定类型的日志信息，这样我们就只看得到报错的日志了。</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127000226816.png" alt="image-20220127000226816"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">创建一个工程:用terminal</span><br><span class="line">scrapy startproject xxx</span><br><span class="line"></span><br><span class="line">在spider字母里中创建一个爬虫文件</span><br><span class="line">·scrapy genspider spiderName www.xxx.com</span><br><span class="line"></span><br><span class="line">执行工程</span><br><span class="line">scrapy crawl spiderName</span><br><span class="line"></span><br><span class="line">LOG_LEVEL = &#x27;ERROR&#x27;</span><br><span class="line">setting里面也有ua伪装</span><br><span class="line"></span><br><span class="line">想要拿到selector里面的data值，就用.extract()对象</span><br><span class="line">如果列表调用了extract，则表示将每一个selector对象中data对应的字符串提取了出来。</span><br></pre></td></tr></table></figure>

<h2 id="5-3数据解析"><a href="#5-3数据解析" class="headerlink" title="5.3数据解析"></a>5.3数据解析</h2><p>下面用一个实例来介绍一下数据解析，爬取一下《碎脸》</p>
<p><a target="_blank" rel="noopener" href="http://www.daomushu.com/suilian/">碎脸小说(鬼古女)_碎脸全文免费阅读_盗墓小说网 (daomushu.com)</a></p>
<p>首先创建创建一个工程，</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127114024171.png" alt="image-20220127114024171"></p>
<p>然后我们就可以用parse方法进行数据解析就可以了</p>
<p>在这个框架中，我们直接在response.xpth()进行数据解析。返回的是一个列表，里面的元素是selector对象，想要拿到里面的data值，就用.extract()对象</p>
<p>那就简单了啊！开始解析！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SuilianSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;suilian&#x27;</span></span><br><span class="line">    <span class="comment">#allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.daomushu.com/suilian/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        url_list = response.xpath(<span class="string">&#x27;//ul/li[@class = &quot;line3&quot;]/a/@href&#x27;</span>)</span><br><span class="line">        new_url = url_list.extract()</span><br><span class="line">        </span><br></pre></td></tr></table></figure>

<p>这样就把所有的url解析出来了，但是我还没有学到用scrapy再次利用url，不可能在这里面再次用一下request模块，就先把这个url给爬下来就可。</p>
<h2 id="5-4数据持久化存储"><a href="#5-4数据持久化存储" class="headerlink" title="5.4数据持久化存储"></a>5.4数据持久化存储</h2><h3 id="5-4-1基于终端指令"><a href="#5-4-1基于终端指令" class="headerlink" title="5.4.1基于终端指令"></a>5.4.1基于终端指令</h3><p>要求只可以将parse方法的返回值存储到本地的文本文件中。</p>
<p><code>scrapy crawl xxx -o ./xxx.csv</code>#在执行的时候-o就可以把parse的返回值存储到当前文件夹下面的xxx.csv中。只支持这些文件类型</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127121921265.png" alt="image-20220127121921265"></p>
<h3 id="5-4-2基于管道（重点）"><a href="#5-4-2基于管道（重点）" class="headerlink" title="5.4.2基于管道（重点）"></a>5.4.2基于管道（重点）</h3><p>编码流程：</p>
<p>1.数据解析</p>
<p>2.在item类中定义相关的属性</p>
<p>3.将解析的数据封装到item类型的对象中</p>
<p>4.将item类型的对象提交给管道进行持久化存储的操作</p>
<p>5.在配置文件中开启管道</p>
<p>在创建的工程中，有一个items文件，在里面给我们定义了一个类</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127122345118.png" alt="image-20220127122345118"></p>
<p>但是并没有定义属性，我们就可以用<code>name = scrapy.Field()</code>来定义我们的属性。</p>
<p>同样也有一个pipelines文件，里面有个类，就是专门用来处理（持久化存储）item对象的。</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127140410577.png" alt="image-20220127140410577"></p>
<p>里面的方法可以接受爬虫文件提交过来的item对象</p>
<p>下面我演示一下处理刚刚得到的碎脸小说url。</p>
<h4 id="5-4-2-1实例"><a href="#5-4-2-1实例" class="headerlink" title="5.4.2.1实例"></a>5.4.2.1实例</h4><p>首先解析到数据</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127140852085.png" alt="image-20220127140852085"></p>
<p>然后在item类中定义相关属性</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127140943953.png" alt="image-20220127140943953"></p>
<p>将解析的数据封装到item类型的对象中（记得导入item类）</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127141502235.png" alt="image-20220127141502235"></p>
<p>将item类型的对象提交给管道，其中processitem方法，只要接收到一个item就会被调用一次</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127141355498.png" alt="image-20220127141355498"></p>
<p>在管道文件中进行持久化操作</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127142717471.png" alt="image-20220127142717471"></p>
<p>在设置中开启管道的注释</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127142335945.png" alt="image-20220127142335945"></p>
<p>三百表示优先级，数值越小优先级越高</p>
<p>最后运行，发现数据以及存储成功</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127142737442.png" alt="image-20220127142737442"></p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127142743267.png" alt="image-20220127142743267"></p>
<h3 id="5-2-3面试题，要把一份数据存到本地一份存到数据库"><a href="#5-2-3面试题，要把一份数据存到本地一份存到数据库" class="headerlink" title="5.2.3面试题，要把一份数据存到本地一份存到数据库"></a>5.2.3面试题，要把一份数据存到本地一份存到数据库</h3><p>那我们可以在管道类中多定义一份类，把他存放到数据库中。</p>
<p>setting中谁的优先级最高谁先接受item，为了保证后面执行的管道类也能够拿到item，在process_Item中也得return一个item。    </p>
<h2 id="5-3全站数据爬取"><a href="#5-3全站数据爬取" class="headerlink" title="5.3全站数据爬取"></a>5.3全站数据爬取</h2><p>就是将网站中某板块下的全部页码对应的页面数据进行爬取。</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127153344429.png" alt="image-20220127153344429"></p>
<p>这样会将所有页面中的xpath表达式表示的数据进行爬爬取。</p>
<h2 id="5-4五大核心组件"><a href="#5-4五大核心组件" class="headerlink" title="5.4五大核心组件"></a>5.4五大核心组件</h2><p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127163002697.png" alt="image-20220127163002697"></p>
<h2 id="5-5请求传参（深度爬取）"><a href="#5-5请求传参（深度爬取）" class="headerlink" title="5.5请求传参（深度爬取）"></a>5.5请求传参（深度爬取）</h2><p>使用场景：如果爬取解析的数据不再同一张页面中</p>
<p>要使用深度爬取就要用</p>
<p><code>yield scrapy.Reuests(xxx,callback=self.zidingyifangfa)</code></p>
<p>为什么不直接用xpath解析呢？因为在首页我们就已经使用了xpath了，为了防止起冲突，我们要额外定义一个方法来实现详情页的数据解析。</p>
<p>那么现在我们就来完善我们的碎脸爬虫</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127165322367.png" alt="image-20220127165322367"></p>
<p>首先我们迭代列表，然后自己写了个方法获取到了小说文本，现在只需要把他们弄到管道存起来就好了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> SUILIAN.items <span class="keyword">import</span> SuilianItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SuilianSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;suilian&#x27;</span></span><br><span class="line">    <span class="comment">#allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.daomushu.com/suilian/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_txt</span>(<span class="params">self,response</span>):</span></span><br><span class="line">        content = response.xpath(<span class="string">&#x27;//div[@id=&quot;content&quot;]//text()&#x27;</span>).extract()</span><br><span class="line">        content = <span class="string">&#x27;&lt;br&gt;&#x27;</span>.join(content)</span><br><span class="line">        <span class="built_in">print</span>(content)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        url_list = response.xpath(<span class="string">&#x27;//ul/li[@class = &quot;line3&quot;]/a/@href&#x27;</span>)</span><br><span class="line">        new_url = url_list.extract()</span><br><span class="line">        <span class="keyword">for</span> iurl <span class="keyword">in</span> new_url:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(iurl,callback=self.parse_txt)</span><br></pre></td></tr></table></figure>

<p>item文件中定义一个新的属性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SuilianItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    content_url = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()<span class="comment">#新定义的属性</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127170831634.png" alt="image-20220127170831634"></p>
<p>在爬虫文件中把内容传入到item类中去,并且给属性赋值，然后将数据提交给管道。</p>
<p>随后我们就爬取到了碎脸小说</p>
<p>下面放一下各个文件的源码</p>
<p>suilian.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> SUILIAN.items <span class="keyword">import</span> SuilianItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SuilianSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;suilian&#x27;</span></span><br><span class="line">    <span class="comment">#allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.daomushu.com/suilian/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_txt</span>(<span class="params">self,response</span>):</span></span><br><span class="line">        content = response.xpath(<span class="string">&#x27;//div[@id=&quot;content&quot;]//text()&#x27;</span>).extract()</span><br><span class="line">        content = <span class="string">&#x27;&lt;br&gt;&#x27;</span>.join(content)</span><br><span class="line">        item = SuilianItem()</span><br><span class="line">        item[<span class="string">&#x27;content&#x27;</span>] = content</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        url_list = response.xpath(<span class="string">&#x27;//ul/li[@class = &quot;line3&quot;]/a/@href&#x27;</span>)</span><br><span class="line">        new_url = url_list.extract()</span><br><span class="line">        <span class="keyword">for</span> iurl <span class="keyword">in</span> new_url:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(iurl,callback=self.parse_txt)</span><br></pre></td></tr></table></figure>

<p>setting</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scrapy settings for SUILIAN project</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># For simplicity, this file contains only settings considered important or</span></span><br><span class="line"><span class="comment"># commonly used. You can find more settings consulting the documentation:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     https://docs.scrapy.org/en/latest/topics/settings.html</span></span><br><span class="line"><span class="comment">#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line"><span class="comment">#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line"></span><br><span class="line">BOT_NAME = <span class="string">&#x27;SUILIAN&#x27;</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">&#x27;SUILIAN.spiders&#x27;</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">&#x27;SUILIAN.spiders&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 Edg/97.0.1072.69&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line">LOG_LEVEL = <span class="string">&#x27;ERROR&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS = 32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure a delay for requests for the same website (default: 0)</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay</span></span><br><span class="line"><span class="comment"># See also autothrottle settings and docs</span></span><br><span class="line"><span class="comment">#DOWNLOAD_DELAY = 3</span></span><br><span class="line"><span class="comment"># The download delay setting will honor only one of:</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS_PER_IP = 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Disable cookies (enabled by default)</span></span><br><span class="line"><span class="comment">#COOKIES_ENABLED = False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Disable Telnet Console (enabled by default)</span></span><br><span class="line"><span class="comment">#TELNETCONSOLE_ENABLED = False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Override the default request headers:</span></span><br><span class="line"><span class="comment">#DEFAULT_REQUEST_HEADERS = &#123;</span></span><br><span class="line"><span class="comment">#   &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;,</span></span><br><span class="line"><span class="comment">#   &#x27;Accept-Language&#x27;: &#x27;en&#x27;,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable or disable spider middlewares</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line"><span class="comment">#SPIDER_MIDDLEWARES = &#123;</span></span><br><span class="line"><span class="comment">#    &#x27;SUILIAN.middlewares.SuilianSpiderMiddleware&#x27;: 543,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable or disable downloader middlewares</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line"><span class="comment">#DOWNLOADER_MIDDLEWARES = &#123;</span></span><br><span class="line"><span class="comment">#    &#x27;SUILIAN.middlewares.SuilianDownloaderMiddleware&#x27;: 543,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable or disable extensions</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/extensions.html</span></span><br><span class="line"><span class="comment">#EXTENSIONS = &#123;</span></span><br><span class="line"><span class="comment">#    &#x27;scrapy.extensions.telnet.TelnetConsole&#x27;: None,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure item pipelines</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;SUILIAN.pipelines.SuilianPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable and configure the AutoThrottle extension (disabled by default)</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_ENABLED = True</span></span><br><span class="line"><span class="comment"># The initial download delay</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_START_DELAY = 5</span></span><br><span class="line"><span class="comment"># The maximum download delay to be set in case of high latencies</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_MAX_DELAY = 60</span></span><br><span class="line"><span class="comment"># The average number of requests Scrapy should be sending in parallel to</span></span><br><span class="line"><span class="comment"># each remote server</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span></span><br><span class="line"><span class="comment"># Enable showing throttling stats for every response received:</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_DEBUG = False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable and configure HTTP caching (disabled by default)</span></span><br><span class="line"><span class="comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span></span><br><span class="line"><span class="comment">#HTTPCACHE_ENABLED = True</span></span><br><span class="line"><span class="comment">#HTTPCACHE_EXPIRATION_SECS = 0</span></span><br><span class="line"><span class="comment">#HTTPCACHE_DIR = &#x27;httpcache&#x27;</span></span><br><span class="line"><span class="comment">#HTTPCACHE_IGNORE_HTTP_CODES = []</span></span><br><span class="line"><span class="comment">#HTTPCACHE_STORAGE = &#x27;scrapy.extensions.httpcache.FilesystemCacheStorage&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>pipelines</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># useful for handling different item types with a single interface</span></span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SuilianPipeline</span>:</span></span><br><span class="line">    fp = <span class="literal">None</span></span><br><span class="line">    <span class="comment">#重写父类的方法，该方法只会被调用一次，不用被多次调用。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self,spider</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;开始爬虫&quot;</span>)</span><br><span class="line">        self.fp = <span class="built_in">open</span>(<span class="string">&#x27;./content.txt&#x27;</span>,mode=<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">        content = item[<span class="string">&#x27;content&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        self.fp.write(<span class="built_in">str</span>(content))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self,spider</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;结束爬虫&#x27;</span>)</span><br><span class="line">        self.fp.close()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>items</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SuilianItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    content_url = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="5-6图片爬取"><a href="#5-6图片爬取" class="headerlink" title="5.6图片爬取"></a>5.6图片爬取</h2><p>在我们的管道类中给我们封装了一个ImagesPipeline</p>
<p>字符串：只需要基于xpath进行解析提交管道进行持久化存储。</p>
<p>图片：xpath解析出图片src的属性值，单独的对图片地址发起请求获取图片二进制类型的数据。    </p>
<p>而我们的ImagesPipeline</p>
<p>只需要那个img的src的属性值进行解析，提交到管道，管道就会对图片的src进行请求发送获取图片的二进制类型的数据，且还会帮我们进行持久化存储。    </p>
<p>使用流程：</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/zhanghongfeng/p/7082510.html">python网络爬虫之使用scrapy爬取图片 - 一张红枫叶 - 博客园 (cnblogs.com)</a></p>
<p>嘿嘿我还是更喜欢使用requests模块，这个scrapy感觉更麻烦了，可能也是我没有掌握到这个框架的精髓，以后再来慢慢学习。</p>
<h1 id="6-分布式爬虫-amp-增量式爬虫"><a href="#6-分布式爬虫-amp-增量式爬虫" class="headerlink" title="6.分布式爬虫&amp;增量式爬虫"></a>6.分布式爬虫&amp;增量式爬虫</h1><h2 id="6-1分布式爬虫"><a href="#6-1分布式爬虫" class="headerlink" title="6.1分布式爬虫"></a>6.1分布式爬虫</h2><p>概念：我们需要搭建一个分布式的机群，让其对一组资源进行分布式联合爬取。</p>
<p>作用：提升爬取数据的效率。</p>
<p>如何实现分部试爬虫？</p>
<p>下载个scrapy-redis的组件</p>
<p><img src="/2022/01/27/py%E7%88%AC%E8%99%AB-%E5%85%A5%E9%97%A8%EF%BC%88b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%89/image-20220127173730289.png" alt="image-20220127173730289"></p>
<p>这个就是基本的一个结构</p>
<p><code>为什么原生的scrapy不可以实现分部式爬虫？</code><br>调度器不可以被分布式机群共享</p>
<p>管道不可以被分布式机群共享</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/xiao-apple36/p/12695092.html">使用scrapy实现分布式爬虫 - 一只小小的寄居蟹 - 博客园 (cnblogs.com)</a></p>
<h2 id="6-2增量式爬虫"><a href="#6-2增量式爬虫" class="headerlink" title="6.2增量式爬虫"></a>6.2增量式爬虫</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/open-yang/p/11343413.html">增量式爬虫 - 笑得好美 - 博客园 (cnblogs.com)</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/01/21/knight-ctf-web-wp/" rel="prev" title="knight ctf web wp">
      <i class="fa fa-chevron-left"></i> knight ctf web wp
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/01/28/%E8%B0%B7%E6%AD%8C%E8%AF%AD%E6%B3%95/" rel="next" title="谷歌语法">
      谷歌语法 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#py%E7%88%AC%E8%99%AB-b%E7%AB%99%E8%AF%BE%E7%A8%8B%EF%BC%88%E5%85%A5%E9%97%A8%E7%9A%84%E7%9F%A5%E8%AF%86%E5%92%8C%E5%9F%BA%E7%A1%80%E7%9A%84%E5%BA%94%E7%94%A8%EF%BC%89"><span class="nav-number">1.</span> <span class="nav-text">py爬虫-b站课程（入门的知识和基础的应用）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E5%88%9D%E8%AF%86requests%E6%A8%A1%E5%9D%97"><span class="nav-number">2.</span> <span class="nav-text">1.初识requests模块</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1%E5%88%86%E7%B1%BB"><span class="nav-number">2.1.</span> <span class="nav-text">1.1分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2http%E5%92%8Chttps"><span class="nav-number">2.2.</span> <span class="nav-text">1.2http和https</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1http%E5%8D%8F%E8%AE%AE"><span class="nav-number">2.2.1.</span> <span class="nav-text">1.2.1http协议</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-1%E5%B8%B8%E7%94%A8%E7%9A%84%E8%AF%B7%E6%B1%82%E5%A4%B4%E4%BF%A1%E6%81%AF"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">1.2.1.1常用的请求头信息</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-2%E5%B8%B8%E7%94%A8%E7%9A%84%E5%93%8D%E5%BA%94%E5%A4%B4%E4%BF%A1%E6%81%AF"><span class="nav-number">2.2.1.2.</span> <span class="nav-text">1.2.1.2常用的响应头信息</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-2https"><span class="nav-number">2.2.2.</span> <span class="nav-text">1.2.2https</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-requests%E6%A8%A1%E5%9D%97"><span class="nav-number">3.</span> <span class="nav-text">2.requests模块</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8"><span class="nav-number">3.1.</span> <span class="nav-text">2.1如何使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2%E6%A0%B9%E6%8D%AE%E6%A1%88%E4%BE%8B%E5%BC%80%E5%A7%8B%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.2.</span> <span class="nav-text">2.2根据案例开始深入学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%80%E6%98%93%E7%9A%84%E7%BD%91%E6%98%93%E9%87%87%E9%9B%86%E5%99%A8%EF%BC%88%E6%A0%B9%E6%8D%AE%E5%85%B3%E9%94%AE%E5%AD%97%E5%9C%A8%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E4%B8%AD%E6%90%9C%E7%B4%A2%EF%BC%89"><span class="nav-number">3.2.1.</span> <span class="nav-text">简易的网易采集器（根据关键字在搜索引擎中搜索）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%B4%E8%A7%A3%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%91"><span class="nav-number">3.2.2.</span> <span class="nav-text">破解百度翻译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3"><span class="nav-number">3.2.3.</span> <span class="nav-text">爬取豆瓣</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%BC%E5%90%88%EF%BC%9A%E5%9B%BD%E5%AE%B6%E8%8D%AF%E5%93%81%E7%9B%91%E7%9D%A3%E7%AE%A1%E7%90%86%E6%80%BB%E5%B1%80%E4%B8%AD%E5%9F%BA%E4%BA%8E%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%90%84%E5%9B%BD%E5%8C%96%E5%A6%86%E5%93%81%E7%94%9F%E4%BA%A7%E8%AE%B8%E5%8F%AF%E7%9B%B8%E5%85%B3%E6%95%B0%E6%8D%AE%EF%BC%88%E5%8A%A8%E6%80%81%E5%8A%A0%E8%BD%BD%EF%BC%89"><span class="nav-number">3.2.4.</span> <span class="nav-text">综合：国家药品监督管理总局中基于中华人民共各国化妆品生产许可相关数据（动态加载）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90"><span class="nav-number">4.</span> <span class="nav-text">2.数据解析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1%E4%BD%BF%E7%94%A8%E6%AD%A3%E5%88%99%E6%9D%A5%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90"><span class="nav-number">4.1.</span> <span class="nav-text">2.1使用正则来进行数据解析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2bs4%E8%A7%A3%E6%9E%90%EF%BC%88python%E7%8B%AC%E6%9C%89%E7%9A%84%E6%96%B9%E5%BC%8F%EF%BC%89"><span class="nav-number">4.2.</span> <span class="nav-text">2.2bs4解析（python独有的方式）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1%E7%A4%BA%E4%BE%8B%EF%BC%9A%E5%AE%9E%E4%BE%8B%E5%8C%96BeautifulSoup"><span class="nav-number">4.2.1.</span> <span class="nav-text">2.2.1示例：实例化BeautifulSoup</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E5%92%8C%E5%B1%9E%E6%80%A7"><span class="nav-number">4.2.2.</span> <span class="nav-text">2.2.2相关方法和属性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3xpath%E8%A7%A3%E6%9E%90"><span class="nav-number">4.3.</span> <span class="nav-text">2.3xpath解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-1%E5%B8%B8%E7%94%A8xpath%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="nav-number">4.3.1.</span> <span class="nav-text">2.3.1常用xpath表达式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E9%AB%98%E6%80%A7%E8%83%BD%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB"><span class="nav-number">5.</span> <span class="nav-text">3.高性能异步爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB%E7%9A%84%E6%96%B9%E5%BC%8F"><span class="nav-number">5.1.</span> <span class="nav-text">3.1异步爬虫的方式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2%E5%8D%95%E7%BA%BF%E7%A8%8B%E5%8A%A0%E5%BC%82%E6%AD%A5%E5%8D%8F%E7%A8%8B"><span class="nav-number">5.2.</span> <span class="nav-text">3.2单线程加异步协程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">5.2.1.</span> <span class="nav-text">3.2.1基本概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C"><span class="nav-number">5.2.2.</span> <span class="nav-text">3.2.2相关操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-3aiohttp%E6%A8%A1%E5%9D%97"><span class="nav-number">5.2.3.</span> <span class="nav-text">3.2.3aiohttp模块</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-selenium%E6%A8%A1%E5%9D%97"><span class="nav-number">6.</span> <span class="nav-text">4.selenium模块</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1%E4%B8%BB%E8%A6%81%E4%BD%9C%E7%94%A8"><span class="nav-number">6.1.</span> <span class="nav-text">4.1主要作用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2selenium%E5%88%9D%E8%AF%95"><span class="nav-number">6.2.</span> <span class="nav-text">4.2selenium初试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95"><span class="nav-number">6.3.</span> <span class="nav-text">4.3常用方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-scrapy%E6%A1%86%E6%9E%B6"><span class="nav-number">7.</span> <span class="nav-text">5.scrapy框架</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85"><span class="nav-number">7.1.</span> <span class="nav-text">5.1环境安装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="nav-number">7.2.</span> <span class="nav-text">5.2基本使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90"><span class="nav-number">7.3.</span> <span class="nav-text">5.3数据解析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8"><span class="nav-number">7.4.</span> <span class="nav-text">5.4数据持久化存储</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-1%E5%9F%BA%E4%BA%8E%E7%BB%88%E7%AB%AF%E6%8C%87%E4%BB%A4"><span class="nav-number">7.4.1.</span> <span class="nav-text">5.4.1基于终端指令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-2%E5%9F%BA%E4%BA%8E%E7%AE%A1%E9%81%93%EF%BC%88%E9%87%8D%E7%82%B9%EF%BC%89"><span class="nav-number">7.4.2.</span> <span class="nav-text">5.4.2基于管道（重点）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-4-2-1%E5%AE%9E%E4%BE%8B"><span class="nav-number">7.4.2.1.</span> <span class="nav-text">5.4.2.1实例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-3%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%8C%E8%A6%81%E6%8A%8A%E4%B8%80%E4%BB%BD%E6%95%B0%E6%8D%AE%E5%AD%98%E5%88%B0%E6%9C%AC%E5%9C%B0%E4%B8%80%E4%BB%BD%E5%AD%98%E5%88%B0%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="nav-number">7.4.3.</span> <span class="nav-text">5.2.3面试题，要把一份数据存到本地一份存到数据库</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3%E5%85%A8%E7%AB%99%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96"><span class="nav-number">7.5.</span> <span class="nav-text">5.3全站数据爬取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4%E4%BA%94%E5%A4%A7%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="nav-number">7.6.</span> <span class="nav-text">5.4五大核心组件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5%E8%AF%B7%E6%B1%82%E4%BC%A0%E5%8F%82%EF%BC%88%E6%B7%B1%E5%BA%A6%E7%88%AC%E5%8F%96%EF%BC%89"><span class="nav-number">7.7.</span> <span class="nav-text">5.5请求传参（深度爬取）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-6%E5%9B%BE%E7%89%87%E7%88%AC%E5%8F%96"><span class="nav-number">7.8.</span> <span class="nav-text">5.6图片爬取</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB-amp-%E5%A2%9E%E9%87%8F%E5%BC%8F%E7%88%AC%E8%99%AB"><span class="nav-number">8.</span> <span class="nav-text">6.分布式爬虫&amp;增量式爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB"><span class="nav-number">8.1.</span> <span class="nav-text">6.1分布式爬虫</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2%E5%A2%9E%E9%87%8F%E5%BC%8F%E7%88%AC%E8%99%AB"><span class="nav-number">8.2.</span> <span class="nav-text">6.2增量式爬虫</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="十三"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">十三</p>
  <div class="site-description" itemprop="description">潜心学习，不骄不躁</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">172</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">十三</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"jsonPath":"live2d-widget-model-tsumiki"},"display":{"position":"right","width":300,"height":600},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>
